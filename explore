#!/usr/bin/Rscript

data = read.delim('ad', header=T)
attach(data)
tapply(sign(score[score >= 3]), font[score >= 3], sum)
tapply(score >= 0, font, sum)
tapply(sign(score[score >= 3]), font[score >= 3], sum) / tapply(score >= 0, font, sum)
tapply(score, font, mean)

# Taking Georgia as the mean, find the mean improvement over Georgia:
tapply(score - mean(score[font=='Georgia']), font, mean)
# or better, with pseudo- matched pairs:
tapply(score - score[font=='Georgia'], font, mean)
# OR:
shuffle <- function(x) { return(sample(x, length(x))) }
tapply(score - shuffle(score[font=='Georgia']), font, mean)

# We can see this yields something (maybe) like a normal curve for each font:
hist(score[font=='Baskerville'] - score[font=='Georgia'], right=FALSE, breaks=-6:6)

# Now if our null hypothesis is that changing the font has no effect, what is the p-value for each font's result?
t.test(score[font=='Baskerville'] - shuffle(score[font=='Georgia']), mu=0)



# Simpler Approach:
# If disagree is 0 and agree is 1, we can find the mean agreement for each font.
# Then our null hypothesis can be that Baskerville has the same mean as Comic Sans.
# In that case, we find the p-value like this:
t.test(score[font=='Baskerville'] >= 3, mu=mean(score[font=='Comic Sans'] >= 3))
# This gives us a p-value of 0.000112, not same as in the article, but very small.
# So it's almost certain the difference is significant.

# We can do something similar with the weighted agree/disagrees,
# using the article's scale of -5, -3, -1, 1, 3, 5:
t.test(score[font=='Baskerville'] * 2 - 5, mu=mean(score[font=='Comic Sans'] * 2 - 5))

# Eh, this is not right. We want to run a two-sample t-test, like this:
t.test(score[font=='Baskerville' | font=='Comic Sans']>=3 ~ font[font=='Baskerville' | font=='Comic Sans'])
# gives a p-value of 0.00661

# and with weighted scores:
t.test(score[font=='Baskerville' | font=='Comic Sans']*2-5 ~ font[font=='Baskerville' | font=='Comic Sans'])
# gives a p-value of 0.003929

# These two results use a Welch t-test, where we measure a separate stddev
# for each sample.
# Here are the results for a Classic t-test, where we assume they have the same stddev:
t(score[font=='Baskerville' | font=='Comic Sans']>=3 ~ font[font=='Baskerville' | font=='Comic Sans'], var.equal=T)
# p-value = 0.006609
t.test(score[font=='Baskerville' | font=='Comic Sans']*2-5 ~ font[font=='Baskerville' | font=='Comic Sans'], var.equal=T)
# p-value = 0.003928

# Now usually a result is "significant" if the p-value is under 0.05 (5%). But here there are 6 fonts, so 6 changes to find an apparently-signficant result. So to prevent data dredging, we ought to require a stricter p-value, such as 0.05 / 6 = 0.0083.
# We can see that the p-values of all our t-tests are indeed lower than this (but not by much).


